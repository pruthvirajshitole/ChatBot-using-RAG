# ğŸ§  RAG Chatbot

A modular Retrieval-Augmented Generation (RAG) chatbot built with **Streamlit**, **Pinecone**, **Sentence Transformers**, and **Groq LLM**. This chatbot retrieves relevant document snippets from your custom data and uses LLMs to generate accurate, context-aware answers.

---

## ğŸ“ Project Structure

```
rag-chatbot/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ ingest.py             # Script to upload data to Pinecone
â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ embedder.py       # Embedding generation using Sentence Transformers
â”‚   â”œâ”€â”€ llm_utils.py      # LLM querying (Groq)
â”‚   â””â”€â”€ pinecone_utils.py # Pinecone DB utilities
â”‚
â””â”€â”€ .streamlit/
    â”œâ”€â”€ secrets.toml      # API keys (never commit this)
    â””â”€â”€ config.toml       # Optional Streamlit configuration
```

---

## âš™ï¸ How It Works

### ğŸ”¹ Data Ingestion (`ingest.py`)

* Reads your source data.
* Generates embeddings using `all-MiniLM-L6-v2`.
* Upserts the embeddings and metadata to **Pinecone**.

### ğŸ”¹ Chat Application (`app.py`)

* Accepts user queries via Streamlit UI.
* Generates embeddings for queries.
* Searches Pinecone for top-matching document chunks.
* Constructs a prompt with the query and retrieved context.
* Sends it to **Groq's `llama3-8b-instant`** for response generation.
* Displays the output to the user.

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/pruthvirajshitole/ChatBot-using-RAG
cd rag-chatbot
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure API Keys

Create a file at `.streamlit/secrets.toml` and add:

```toml
[general]
PINECONE_API_KEY = "your-pinecone-api-key"
GROQ_API_KEY = "your-groq-api-key"
```

> âš ï¸ **Never commit your `secrets.toml` to version control.**

### 4. Ingest Your Data

```bash
python ingest.py
```

### 5. Launch the Chatbot

```bash
streamlit run app.py
```

Visit the local Streamlit URL to start chatting.

---

## ğŸ§± Tech Stack

* ğŸ§  **LLM**: Groq API (`llama3-8b-instant`)
* ğŸ“† **Vector DB**: Pinecone
* ğŸ” **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`
* ğŸ–¼ï¸ **Frontend**: Streamlit
* ğŸ› ï¸ **Language**: Python

---

## ğŸ“Š RAG Pipeline Overview

```mermaid
flowchart TD
    A["User Input (Query & Context via Streamlit UI)"] --> B["Generate Embeddings (Query & Context) via Hugging Face"]
    B --> C["Query Pinecone for Similar Context"]
    C --> D["Extract Context from Top Matches"]
    D --> E["Construct Prompt for LLM (with user query & chosen context)"]
    E --> F["Send Prompt to Groq LLM API"]
    F --> G["Display LLM Response in UI"]
```

---

## ğŸš€ Deployment

* For **local use**, Streamlit is sufficient.
* For ğŸš€ **Cloud Deployment**, you can host on platforms like:
- [Streamlit Community Cloud](https://streamlit.io/cloud)
- [Render](https://render.com)
- [Railway](https://railway.app)

ğŸ‘‰ **Live App:** [Click here to try the RAG Chatbot](https://chatbot-using-rag-techonsy.streamlit.app/)

---

## ğŸ“Œ Notes

* Modular design allows swapping out vector DBs or LLMs.
* `.env` is no longer used; all secrets go in `.streamlit/secrets.toml`.
* Easily extendable for PDFs, websites, or Notion content ingestion.

---

## ğŸ’¡ Future Improvements

* Support for PDF/Text/CSV ingestion
* OpenAI or Gemini as optional LLM backends
* Document preview panel
* Chat history saving

---

## ğŸ“ License

MIT License. See `LICENSE` file for details.
